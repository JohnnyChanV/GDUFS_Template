% 1
@article{li2022midtd,
  title={Midtd: A simple and effective distillation framework for distantly supervised relation extraction},
  author={Li, Rui and Yang, Cheng and Li, Tingwei and Su, Sen},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={40},
  number={4},
  pages={1--32},
  year={2022},
  publisher={ACM New York, NY}
}
% 2
@article{christou2021improving,
  title={Improving distantly-supervised relation extraction through bert-based label and instance embeddings},
  author={Christou, Despina and Tsoumakas, Grigorios},
  journal={IEEE Access},
  volume={9},
  pages={62574--62582},
  year={2021},
  publisher={IEEE}
}
% 3
@inproceedings{socher2012semantic,
  title={Semantic compositionality through recursive matrix-vector spaces},
  author={Socher, Richard and Huval, Brody and Manning, Christopher D and Ng, Andrew Y},
  booktitle={Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning},
  pages={1201--1211},
  year={2012}
}
% 4
@article{zeng2018adversarial,
  title={Adversarial learning for distant supervised relation extraction},
  author={Zeng, Daojian and Dai, Yuan and Li, Feng and Sherratt, R Simon and Wang, Jin},
  journal={Computers, Materials \& Continua},
  volume={55},
  number={1},
  pages={121--136},
  year={2018},
  publisher={Tech Science Press}
}
% 5
@inproceedings{zhou2016attention,
  title={Attention-based bidirectional long short-term memory networks for relation classification},
  author={Zhou, Peng and Shi, Wei and Tian, Jun and Qi, Zhenyu and Li, Bingchen and Hao, Hongwei and Xu, Bo},
  booktitle={Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short papers)},
  pages={207--212},
  year={2016}
}
% 6
@inproceedings{riedel2010modeling,
  title={Modeling relations and their mentions without labeled text},
  author={Riedel, Sebastian and Yao, Limin and McCallum, Andrew},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part III 21},
  pages={148--163},
  year={2010},
  organization={Springer}
}
% 7
@inproceedings{ye2019distant,
    title = "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions",
    author = "Ye, Zhi-Xiu  and
      Ling, Zhen-Hua",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1288",
    doi = "10.18653/v1/N19-1288",
    pages = "2810--2819",
    abstract = "This paper presents a neural relation extraction method to deal with the noisy training data generated by distant supervision. Previous studies mainly focus on sentence-level de-noising by designing neural networks with intra-bag attentions. In this paper, both intra-bag and inter-bag attentions are considered in order to deal with the noise at sentence-level and bag-level respectively. First, relation-aware bag representations are calculated by weighting sentence embeddings using intra-bag attentions. Here, each possible relation is utilized as the query for attention calculation instead of only using the target relation in conventional methods. Furthermore, the representation of a group of bags in the training set which share the same relation label is calculated by weighting bag representations using a similarity-based inter-bag attention module. Finally, a bag group is utilized as a training sample when building our relation extractor. Experimental results on the New York Times dataset demonstrate the effectiveness of our proposed intra-bag and inter-bag attention modules. Our method also achieves better relation extraction accuracy than state-of-the-art methods on this dataset.",
}
% 8
@article{vahdat2017toward,
  title={Toward robustness against label noise in training deep discriminative neural networks},
  author={Vahdat, Arash},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
% 9
@article{yao2018deep,
  title={Deep learning from noisy image labels with quality embedding},
  author={Yao, Jiangchao and Wang, Jiajie and Tsang, Ivor W and Zhang, Ya and Sun, Jun and Zhang, Chengqi and Zhang, Rui},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={4},
  pages={1909--1922},
  year={2018},
  publisher={IEEE}
}
% 10
@inproceedings{ghosh2017robust,
  title={Robust loss functions under label noise for deep neural networks},
  author={Ghosh, Aritra and Kumar, Himanshu and Sastry, P Shanti},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}
% 11
@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{li2020dividemix,
  title={DivideMix: Learning with Noisy Labels as Semi-supervised Learning},
  author={Junnan Li and Richard Socher and Steven C. H. Hoi},
  journal={ArXiv},
  year={2020},
  volume={abs/2002.07394}
}
% 13
@inproceedings{tanaka2018joint,
  title={Joint optimization framework for learning with noisy labels},
  author={Tanaka, Daiki and Ikami, Daiki and Yamasaki, Toshihiko and Aizawa, Kiyoharu},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5552--5560},
  year={2018}
}
% 14
@inproceedings{jiang2018mentornet,
  title={Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels},
  author={Jiang, Lu and Zhou, Zhengyuan and Leung, Thomas and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International conference on machine learning},
  pages={2304--2313},
  year={2018},
  organization={PMLR}
}
% 15
@inproceedings{shi2018genre,
  title={Genre separation network with adversarial training for cross-genre relation extraction},
  author={Shi, Ge and Feng, Chong and Huang, Lifu and Zhang, Boliang and Ji, Heng and Liao, Lejian and Huang, He-Yan},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={1018--1023},
  year={2018}
}
% 16
@inproceedings{wu2017adversarial,
  title={Adversarial training for relation extraction},
  author={Wu, Yi and Bamman, David and Russell, Stuart},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={1778--1783},
  year={2017}
}
% 17
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
% 18
@inproceedings{li2020self,
  title={Self-attention enhanced selective gate with entity-aware embedding for distantly supervised relation extraction},
  author={Li, Yang and Long, Guodong and Shen, Tao and Zhou, Tianyi and Yao, Lina and Huo, Huan and Jiang, Jing},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8269--8276},
  year={2020}
}
% 19
@inproceedings{yi2019probabilistic,
  title={Probabilistic end-to-end noise correction for learning with noisy labels},
  author={Yi, Kun and Wu, Jianxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7017--7025},
  year={2019}
}
% 20
@inproceedings{lin2016neural,
  title={Neural relation extraction with selective attention over instances},
  author={Lin, Yankai and Shen, Shiqi and Liu, Zhiyuan and Luan, Huanbo and Sun, Maosong},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2124--2133},
  year={2016}
}
% 21
@inproceedings{liu2017soft,
  title={A soft-label method for noise-tolerant distantly supervised relation extraction},
  author={Liu, Tianyu and Wang, Kexiang and Chang, Baobao and Sui, Zhifang},
  booktitle={Proceedings of the 2017 conference on empirical methods in natural language processing},
  pages={1790--1795},
  year={2017}
}
% 22
@article{lin2017structured,
  title={A Structured Self-attentive Sentence Embedding},
  author={Zhouhan Lin and Minwei Feng and C{\'i}cero Nogueira dos Santos and Mo Yu and Bing Xiang and Bowen Zhou and Yoshua Bengio},
  journal={ArXiv},
  year={2017},
  volume={abs/1703.03130}
}
% 23
@inproceedings{han2018hierarchical,
  title={Hierarchical relation extraction with coarse-to-fine grained attention},
  author={Han, Xu and Yu, Pengfei and Liu, Zhiyuan and Sun, Maosong and Li, Peng},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2236--2245},
  year={2018}
}
@inproceedings{alt2019fine,
    title = "Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction",
    author = {Alt, Christoph  and
      H{\"u}bner, Marc  and
      Hennig, Leonhard},
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1134",
    doi = "10.18653/v1/P19-1134",
    pages = "1388--1398",
    abstract = "Distantly supervised relation extraction is widely used to extract relational facts from text, but suffers from noisy labels. Current relation extraction methods try to alleviate the noise by multi-instance learning and by providing supporting linguistic and contextual information to more efficiently guide the relation classification. While achieving state-of-the-art results, we observed these models to be biased towards recognizing a limited set of relations with high precision, while ignoring those in the long tail. To address this gap, we utilize a pre-trained language model, the OpenAI Generative Pre-trained Transformer (GPT) (Radford et al., 2018). The GPT and similar models have been shown to capture semantic and syntactic features, and also a notable amount of {``}common-sense{''} knowledge, which we hypothesize are important features for recognizing a more diverse set of relations. By extending the GPT to the distantly supervised setting, and fine-tuning it on the NYT10 dataset, we show that it predicts a larger set of distinct relation types with high confidence. Manual and automated evaluation of our model shows that it achieves a state-of-the-art AUC score of 0.422 on the NYT10 dataset, and performs especially well at higher recall levels.",
}
@inproceedings{chen2021cil,
    title = "{CIL}: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction",
    author = "Chen, Tao  and
      Shi, Haizhou  and
      Tang, Siliang  and
      Chen, Zhigang  and
      Wu, Fei  and
      Zhuang, Yueting",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.483",
    doi = "10.18653/v1/2021.acl-long.483",
    pages = "6191--6200",
    abstract = "The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.",
}

@inproceedings{li2022hiclre,
    title = "{H}i{CLRE}: A Hierarchical Contrastive Learning Framework for Distantly Supervised Relation Extraction",
    author = "Li, Dongyang  and
      Zhang, Taolin  and
      Hu, Nan  and
      Wang, Chengyu  and
      He, Xiaofeng",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.202",
    doi = "10.18653/v1/2022.findings-acl.202",
    pages = "2567--2578",
    abstract = "Distant supervision assumes that any sentence containing the same entity pairs reflects identical relationships. Previous works of distantly supervised relation extraction (DSRE) task generally focus on sentence-level or bag-level de-noising techniques independently, neglecting the explicit interaction with cross levels. In this paper, we propose a hierarchical contrastive learning Framework for Distantly Supervised relation extraction (HiCLRE) to reduce noisy sentences, which integrate the global structural information and local fine-grained interaction. Specifically, we propose a three-level hierarchical learning framework to interact with cross levels, generating the de-noising context-aware representations via adapting the existing multi-head self-attention, named Multi-Granularity Recontextualization. Meanwhile, pseudo positive samples are also provided in the specific level for contrastive learning via a dynamic gradient-based data augmentation strategy, named Dynamic Gradient Adversarial Perturbation. Experiments demonstrate that HiCLRE significantly outperforms strong baselines in various mainstream DSRE datasets.",
}

@inproceedings{mintz2009distant,
  title={Distant supervision for relation extraction without labeled data},
  author={Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
  pages={1003--1011},
  year={2009}
}

@inproceedings{soares2019matching,
    title = "Matching the Blanks: Distributional Similarity for Relation Learning",
    author = "Baldini Soares, Livio  and
      FitzGerald, Nicholas  and
      Ling, Jeffrey  and
      Kwiatkowski, Tom",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1279",
    doi = "10.18653/v1/P19-1279",
    pages = "2895--2905",
    abstract = "General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction. Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph. However, both of these approaches are limited in their ability to generalize. In this paper, we build on extensions of Harris{'} distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text. We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task{'}s training data. We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on SemEval 2010 Task 8, KBP37, and TACRED",
}

@inproceedings{vashishth2018reside,
    title = "{RESIDE}: Improving Distantly-Supervised Neural Relation Extraction using Side Information",
    author = "Vashishth, Shikhar  and
      Joshi, Rishabh  and
      Prayaga, Sai Suman  and
      Bhattacharyya, Chiranjib  and
      Talukdar, Partha",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1157",
    doi = "10.18653/v1/D18-1157",
    pages = "1257--1266",
    abstract = "Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE{'}s effectiveness. We have made RESIDE{'}s source code available to encourage reproducible research.",
}


@inproceedings{li2019chinese,
  title={Chinese relation extraction with multi-grained information and external linguistic knowledge},
  author={Li, Ziran and Ding, Ning and Liu, Zhiyuan and Zheng, Haitao and Shen, Ying},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4377--4386},
  year={2019}
}

@article{jat2018improving,
  title={Improving distantly supervised relation extraction using word and entity based attention},
  author={Jat, Sharmistha and Khandelwal, Siddhesh and Talukdar, Partha},
  journal={arXiv preprint arXiv:1804.06987},
  year={2018}
}

@article{xu2017discourse,
  title={A discourse-level named entity recognition and relation extraction dataset for chinese literature text},
  author={Xu, Jingjing and Wen, Ji and Sun, Xu and Su, Qi},
  journal={arXiv preprint arXiv:1711.07010},
  year={2017}
}

@mastersthesis{chen2015convolutional,
  title={Convolutional neural network for sentence classification},
  author={Chen, Yahui},
  year={2015},
  school={University of Waterloo}
}

@article{zhang2015sensitivity,
  title={A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification},
  author={Zhang, Ye and Wallace, Byron},
  journal={arXiv preprint arXiv:1510.03820},
  year={2015}
}